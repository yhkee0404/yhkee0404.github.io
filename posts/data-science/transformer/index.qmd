---
title: "Transformer에서 배울 점"
author: "Yunho Kee"
date: "2025-05-25"
categories:
  - "data science"
  - transformer
  - rnn
  - gpt
  - encoder
  - decoder
  - stateless
  - "auto-regressive"
  - normalization
  - residual
image: /_images/thumbnails/mohit-kumar-6M9xiVgkoN0-unsplash.jpg
bibliography: 
  - vaswani2023attentionneed.bib
  - li2019positionalnormalization.bib
  - commit_im2025layer_normalization.bib
---

![Photo by <a href="https://unsplash.com/@98mohitkumar?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash">Mohit Kumar</a> on <a href="https://unsplash.com/photos/aerial-photography-of-buildings-6M9xiVgkoN0?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash">Unsplash</a>]({{< meta image >}}){fig-alt="A photo of a bird's eye view of a city."}

Transformer 논문에서 배울 점은 무엇일까?

1. RNN(Recurrent Neural Networks)과 Transformer의 비교

  RNN(Recurrent Neural Networks)은 입력뿐 아니라 입력의 앞부분에 대해 생성한 상태에 의존해 출력한다.
  반면에 Transformer는 **Encoder의 전처리** 덕분에 RNN(Recurrent Neural Networks)과 달리 그리고 마치 HTTP처럼 Stateless하다.
  그래서 학습을 마치고 추론할 때도 RNN(Recurrent Neural Networks)과 달리 입력의 앞부분에 대한 상태 생성을 건너뛸 수 있다.
  또한 Encoder 덕분에 Decoder의 각 출력에 대해 RNN(Recurrent Neural Networks)처럼 직렬 학습하는 대신 병렬 학습이 가능하다.

2. GPT(Generative Pre-trained Transformer)와 Transformer의 비교

  GPT(Generative Pre-trained Transformer)의 입력은 출력에 Auto-Regressive하다.
  반면에 Transformer의 Decoder는 GPT(Generative Pre-trained Transformer)와 달리 입력에 Encoder의 출력으로서 **Auto-Regressive하지 않은 입력**도 포함한다.

3. Transformer의 Encoder와 Decoder의 시간 복잡도 차이

  Transformer의 Encoder와 Decoder는 둘다 $N = 6$개의 Layer의 Stack으로 구성된다 [@vaswani2023attentionneed].
  그러나 Encoder는 입력 길이 전체에 대해 **1번만 전처리**하는 반면에 Decoder는 Auto-Regressive하므로 **출력 길이만큼** 수행해야 한다.
  다만 Decoder의 학습은 학습을 마치고 추론할 때와 달리 **병렬 처리**가 가능하다.

4. Layer Normalization과 비교

  Normalization이란 표준화를 의미한다 [see @commit_im2025layer_normalization, 48 s]. Data의 Sample 또는 Instance는 Batch라는 더 큰 단위를 이루거나 또는 반대로 Channel 단위로 나뉠 수도 있다. Channel 단위로 나눈다면 **Batch Normalization**과 **Instance Normalization**은 각각 Batch 단위와 Instance 단위로 계산하면서도 내부적으로는 **Channel 단위**로 표준화한다 [see @li2019positionalnormalization]. **Layer Normalization**과 **Group Normalization**, **Positional Normalization**은 모두 Instance 단위로 계산하면서도 각각 내부적으로는 Layer에서 **Channel**과 **공간적인 Position**을 구분하지 않고 표준화하고, 나머지 둘은 **Channel**을 구분하지 않으면서도 전자는 Channel의 **Group** 단위로, 후자는 **공간적인 Position** 단위로 표준화한다 [see @li2019positionalnormalization].
  
  Transformer의 입력은 **시간적인 Position**과 그 **하위 차원**으로 나눌 수 있다. 그리고 Transformer의 **Layer Normalization**은 **시간적인 Position** 단위로 표준화하므로 반대로 구분하지 않아야 할 **Channel**이나 **공간적인 Position**은 **시간적인 Position**과 별개이다. 이때 **하위 차원**을 **Channel**로 볼 수 있는 2가지 근거가 있다. 첫째는, **Fully-Connected Feed-Forward Networks**를 Kernel Size가 1인 2개의 **Convolutions**로 이해하려면 하위 차원이 **Channel**이어야 하기 때문이다. 그리고 **Convolution**의 **Softmax**가 Channel 간 가중 평균 확률을 구할 때처럼 Transformer의 **Softmax**도 Query 행렬의 **시간적인 Position**마다 **하위 차원**인 Key 행렬의 **시간적인 Position**, 또는 Value 행렬의 **시간적인 Position**에 대해 계산하기 때문이다. 그러면 Transformer의 입력에서 **공간적인 Position**은 없거나 1개라고 생각할 수 있다.
  
  만일 Transformer의 **Layer Normalization**에서 구분하지 않는 **하위 차원**을 **공간적인 Position**이라고 생각해 보면 반대로 **공간적인 Position**을 구분하는 **Positional Normalization**이라고는 설명할 수 없다. 대신에 **시간적인 Position**을 **공간적인 Position**으로 간주하면 **Positional Normalization**이라고 생각할 수도 있고, **하위 차원**은 **Channel**이라고 볼 수 있다. 그러면 반대로 **공간적인 Position** 단위로 표준화하지 않는 **Layer Normalization**이라고는 더 이상 설명하지 못하게 된다는 문제가 있다 [see @commit_im2025layer_normalization, 48 s].

5. Residual Connections

  Layer들의 **집합**이 $N$개일 때 **Residual Connections**를 통해 출력에 $N$개의 **집합** 1개 대신 그 집합의 **모든 부분집합** $2^N$개가 기여할 수 있다.